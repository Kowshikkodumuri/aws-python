What is the S3?
# Amazon S3 (Simple Storage Service) is a scalable, high-speed, web-based cloud storage service designed for object storage.
# It is a core part of Amazon Web Services (AWS) and is used by individuals, developers, and businesses for storing and retrieving any amount of data at any time, from anywhere on the web.
# Amazon S3 is an object storage service that allows you to store and retrieve any amount of data on the internet. It’s built to provide 99.999999999% (11 9’s) durability and high availability.

S3 Storage Classes
--------------------
Class Name			Use Case								Durability		Availability		Retrieval Time
----------                     ---------------                                                        -------------            ----------------         ----------------
S3 Standard	          Frequently accessed data							99.999999999%		99.99%			Milliseconds
S3 Intelligent-Tiering	  Optimizes costs by moving data between tiers based on access patterns		99.999999999%		99.9–99.99%		Milliseconds
S3 Standard-IA		  Infrequently accessed, long-lived data					99.999999999%		99.9%			Milliseconds
S3 One Zone-IA		  Infrequently accessed, not mission-critical (stored in 1 AZ)			99.999999999%		99.5%			Milliseconds
S3 Glacier		  Archive storage with flexible retrieval times					99.999999999%		99.99%			Minutes to hours
S3 Glacier Deep Archive	  Lowest cost archive, for long-term storage					99.999999999%		99.99%			Hours

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
WHAT IS IAM?
# IAM (Identity and Access Management) is a service provided by AWS that helps you manage users, groups, roles, and their permissions to securely access AWS services and resources.
# IAM allows you to control who can access specific AWS resources, and what actions they can perform on those resources, using policies.

With IAM, you can manage:
--------------------------
# Users: Individuals or systems that need to access AWS resources.
# Groups: A collection of users with similar permissions.
# Roles: A set of permissions that can be assumed by users, applications, or services (like EC2 instances, Lambda functions, etc.).
# Policies: Documents that define permissions, usually in JSON format.

Key Concepts in IAM
----------------------
IAM Users:
-------------
# Definition: An IAM user is an entity within your AWS account that represents a person or application. Each IAM user has a unique set of security credentials (username and password, access keys) to authenticate to AWS.
# Use Case: If an individual needs access to AWS resources, they get an IAM user.

IAM Groups:
------------
# Definition: An IAM group is a collection of IAM users. You assign permissions to the group, and all users in the group inherit those permissions.
# Use Case: Groups are used to manage permissions for multiple users more efficiently. For example, you might have an "Admins" group with full access to all AWS resources and a "Developers" group with limited access.

IAM Roles:
------------
# Definition: An IAM role is an AWS identity with specific permissions that can be assumed by trusted entities like IAM users, AWS services (e.g., EC2, Lambda), or other accounts.
# Use Case: You use IAM roles when you need to grant temporary access to AWS resources. For instance, an EC2 instance may assume a role to access an S3 bucket or a Lambda function might assume a role to write to DynamoDB.

IAM Policies:
--------------
# Definition: A policy is a document that defines permissions for a particular resource. Policies are written in JSON and specify what actions are allowed or denied on which resources.
# Use Case: Policies are attached to IAM users, groups, or roles. They specify the level of access (such as read-only, full access, or custom) to AWS resources.

IAM Policy Types
----------------
There are different types of IAM policies:

AWS Managed Policies:
------------------------
These are pre-defined policies provided by AWS that grant permissions for common use cases. For example:

# AdministratorAccess: Full access to all AWS services and resources.
--------------------
# AmazonS3FullAccess: Full access to Amazon S3.
-------------------

# Customer Managed Policies:
---------------------------
These are custom policies created by you to grant specific permissions that suit your needs. You write these policies based on your use case.

# Inline Policies:
------------------
These are policies that are directly embedded into a single IAM user, group, or role. They are not reusable across multiple entities.


How IAM Works
------------------
Authentication: This is the process of verifying who the user or service is (using AWS credentials such as access keys or a session token).
----------------
Authorization: Once authenticated, IAM checks if the user or service is authorized to perform the requested action. This is done by checking attached policies (user, group, role, or resource-based).
---------------


WHAT IS LAMBADA?
AWS Lambda: Serverless compute service that runs code in response to events.
-----------
Lambda Triggering: Events (e.g., from S3, DynamoDB, API Gateway) that initiate Lambda function execution.
------------------
Lambda Layers: Shared code and dependencies to keep Lambda functions lightweight.
-------------
Use Cases: File processing, real-time data processing, serverless apps, automation, etc.
------------
Test Events: Simulated events to test Lambda function behavior before real triggers.
-------------

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
TASK-1 Automatically copy files from one Bucket to another Bucket
---------------------------------------------------------------------
LAMBDA CODE
--------------

import boto3

s3 = boto3.resource('s3')
source_bucket = s3.Bucket('firstbucketfiles2')
destination_bucket = s3.Bucket('secondbucketfiles3')

def lambda_handler(event, context):
    # Use empty string to list all objects
    for obj in source_bucket.objects.filter(Prefix=''):
        source_key = obj.key
        print(f"Copying file {source_key}")
        
        copy_source = {
            'Bucket': 'firstbucketfiles2',
            'Key': source_key
        }
        
        destination_bucket.copy(copy_source, source_key)
        
        # obj.delete()  # Commented out to prevent deletion



IAM ROLE --> CREATED POLICY 
---------------------------------
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowReadFromSource",
            "Effect": "Allow",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::firstbucketfiles2/*"
        },
        {
            "Sid": "AllowListSource",
            "Effect": "Allow",
            "Action": "s3:ListBucket",
            "Resource": "arn:aws:s3:::firstbucketfiles2"
        },
        {
            "Sid": "AllowWriteToDest",
            "Effect": "Allow",
            "Action": "s3:PutObject",
            "Resource": "arn:aws:s3:::secondbucketfiles3/*"
        }
    ]
}

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
TASK-2 One folder to another folder within the bucket.
----------------------------------------------------------------
LAMBDA CODE
---------------
import boto3

s3 = boto3.resource('s3')
bucket_name = 'mybucketcopyfiles'  # Your bucket name
bucket = s3.Bucket(bucket_name)

def lambda_handler(event, context):
    source_folder = 'folder1/'        # Folder to copy from
    destination_folder = 'folder2/'   # Folder to copy to

    # Iterate over all objects in folder1
    for obj in bucket.objects.filter(Prefix=source_folder):
        source_key = obj.key

        # Skip if it's a folder placeholder
        if source_key.endswith('/'):
            continue

        # Construct the destination key by replacing source folder with destination folder
        destination_key = source_key.replace(source_folder, destination_folder, 1)

        # Create the copy source dictionary
        copy_source = {
            'Bucket': bucket_name,
            'Key': source_key
        }

        print(f"Copying {source_key} to {destination_key}")
        # Copy the object to the destination folder
        bucket.copy(copy_source, destination_key)


IAM ROLE --> CREATED POLICY 
-----------------------------
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowListAndReadWriteWithinBucket",
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket"
            ],
            "Resource": "arn:aws:s3:::mybucketcopyfiles"
        },
        {
            "Sid": "AllowGetPut",
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:PutObject"
            ],
            "Resource": "arn:aws:s3:::mybucketcopyfiles/*"
        }
    ]
}

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------


# Cold Start: Takes longer due to the initialization of the Lambda environment.
--------------
# Warm Start: Executes faster because the environment is already initialized.
--------------






















